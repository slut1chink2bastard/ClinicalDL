{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we will change a few settings to make the notebook look a bit prettier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> body {font-family: \"Calibri\", cursive, sans-serif;} </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style> body {font-family: \"Calibri\", cursive, sans-serif;} </style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 00 - Understanding DeepSurv (using Keras)\n",
    "Before anything else, it makes sense to spend some time in understanding\n",
    "how the original DeepSurv works. In this notebook, we take an example dataset\n",
    "and go step by step through the algorithm. Please note that the code \n",
    "here was written with clarity over performance in mind.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, ActivityRegularization\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from lifelines import utils\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_file = '00_understanding_deepsurv'\n",
    "PATH_DATA = pathlib.Path(r'../deepsurvk/datasets/data')\n",
    "PATH_MODELS = pathlib.Path('./models/')\n",
    "\n",
    "# Make sure data directory exists.\n",
    "if not PATH_DATA.exists():\n",
    "    raise ValueError(f\"The directory {PATH_DATA} does not exist.\")\n",
    "\n",
    "# If models directory does not exist, create it.\n",
    "if not PATH_MODELS.exists():\n",
    "    print(\"Creating models directory in \" + str(PATH_MODELS) + \"...\\t\", end=\"\", flush=True)\n",
    "    PATH_MODELS.mkdir(parents=True)\n",
    "    print(\"DONE!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "In this case, we will use the Worcester Heart Attack Study (WHAS) dataset.\n",
    "For a more detailed description about it, please see the corresponding\n",
    "[README](../data/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "path_data_file = PATH_DATA/'whas.h5'\n",
    "\n",
    "# Read training data.\n",
    "with h5py.File(path_data_file, 'r') as f:\n",
    "    X_train = f['train']['x'][()]\n",
    "    E_train = f['train']['e'][()]\n",
    "    Y_train = f['train']['t'][()].reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Read testing data.\n",
    "with h5py.File(path_data_file, 'r') as f:\n",
    "    X_test = f['test']['x'][()]\n",
    "    E_test = f['test']['e'][()]\n",
    "    Y_test = f['test']['t'][()].reshape(-1, 1)\n",
    "\n",
    "# Calculate important parameters.\n",
    "n_patients_train = X_train.shape[0]\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data\n",
    "* Standardization <br>\n",
    "First, we need to standardize the input (p. 3).\n",
    "Notice how we only use training data for the standardization.\n",
    "This done to avoid leakage (using information from\n",
    "the testing partition for the model training.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train = X_scaler.transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "\n",
    "Y_scaler = StandardScaler().fit(Y_train.reshape(-1, 1))\n",
    "Y_train = Y_scaler.transform(Y_train)\n",
    "Y_test = Y_scaler.transform(Y_test)\n",
    "\n",
    "Y_train = Y_train.flatten()\n",
    "Y_test = Y_test.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sorting <br>\n",
    "This is important, since we are performing a ranking task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(Y_train)[::-1]\n",
    "X_train = X_train[sort_idx]\n",
    "Y_train = Y_train[sort_idx]\n",
    "E_train = E_train[sort_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "DeepSurv's loss function is the average negative log partial likelihood with\n",
    "regularization (Eq. 4, p. 3):\n",
    "   \n",
    "$$l_{\\theta} = -\\frac{1}{N_{E=1}} \\sum_{i:E_i=1} \\left( \\hat{h}_\\theta(x_i) -\\log \\sum_{j \\in {\\rm I\\!R}(T_i)} \\exp^{\\hat{h}_\\theta(x_j)} \\right) + \\lambda \\cdot \\Vert \\theta \\Vert_2^2 $$\n",
    "\n",
    "We can see that our loss function depends on three parameters:\n",
    "`y_true`, `y_pred`, *and* `E`. Unfortunately, custom loss functions in Keras\n",
    "[need to have their signature (i.e., prototype) as](https://keras.io/api/losses/#creating-custom-losses)\n",
    "`loss_fn(y_true, y_pred)`. To overcome this, we will use a [small trick](https://github.com/keras-team/keras/issues/2121)\n",
    "that is actually well known in the community. This way, we can define the \n",
    "negative log likelihood function as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(E):\n",
    "    def loss(y_true, y_pred):\n",
    "        \n",
    "        hazard_ratio = tf.math.exp(y_pred)        \n",
    "        log_risk = tf.math.log(tf.math.cumsum(hazard_ratio))\n",
    "        uncensored_likelihood = tf.transpose(y_pred) - log_risk\n",
    "        censored_likelihood = uncensored_likelihood * E\n",
    "        neg_likelihood_ = -tf.math.reduce_sum(censored_likelihood)\n",
    "\n",
    "        # TODO\n",
    "        # For some reason, adding num_observed_events does not work.\n",
    "        # Therefore, for now we will use it as a simple factor of 1.\n",
    "        # Is it really needed? Isn't it just a scaling factor?\n",
    "        # num_observed_events = tf.math.cumsum(E)\n",
    "        # num_observed_events = tf.cast(num_observed_events, dtype=tf.float32)\n",
    "        num_observed_events = tf.constant(1, dtype=tf.float32)\n",
    "        \n",
    "        neg_likelihood = neg_likelihood_ / num_observed_events        \n",
    "        \n",
    "        return neg_likelihood\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with regularization added further on (as part of the network architecture).\n",
    "\n",
    "## Define model parameters\n",
    "Nothing spectacular here. You can see these are pretty standard parameters.\n",
    "We will use the values reported in Table 2 (p. 10).\n",
    "\n",
    "If you decide to try a different dataset, be sure to change these\n",
    "accordingly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "activation = 'relu'\n",
    "n_nodes = 48\n",
    "learning_rate = 0.067\n",
    "l2_reg = 16.094\n",
    "dropout = 0.147\n",
    "lr_decay =  6.494e-4\n",
    "momentum = 0.863"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model construction\n",
    "Now we can build the model. We will do this using the `Sequential` \n",
    "constructor, thus adding layer by layer.\n",
    "\n",
    "The initialization of the nodes weights can be done in many different\n",
    "ways. In the original DeepSurv implementation, they used [Glorot\n",
    "with weights sampled from the uniform distribution](https://github.com/jaredleekatzman/DeepSurv/blob/198bb2375ea2d2cad93e568ffc550889366494ef/deepsurv/deep_surv.py#L78),\n",
    "as proposed by [Glorot and Bengio (2010)](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).\n",
    "Therefore, we will stick with that initialization as well.\n",
    "\n",
    "Notice that this architecture works for the sample dataset (WHAS).\n",
    "It is slightly different for each dataset (mainly the optimizer and \n",
    "number of hidden layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=n_features, activation=activation, kernel_initializer='glorot_uniform', input_shape=(n_features,)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(units=n_nodes, activation=activation, kernel_initializer='glorot_uniform'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(units=n_nodes, activation=activation, kernel_initializer='glorot_uniform'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(units=1, activation='linear', kernel_initializer='glorot_uniform', kernel_regularizer=l2(l2_reg)))\n",
    "model.add(ActivityRegularization(l2=l2_reg))\n",
    "\n",
    "# Define the optimizer\n",
    "# Nadam is Adam + Nesterov momentum\n",
    "# optimizer = Nadam(learning_rate=learning_rate, decay=lr_decay, clipnorm=1) \n",
    "optimizer = Nadam(learning_rate=learning_rate, decay=lr_decay)\n",
    "\n",
    "# Compile the model and show a summary of it\n",
    "model.compile(loss=negative_log_likelihood(E_train), optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, the computation of the loss yields a `NaN`, which makes the whole\n",
    "output be `NaN` as well. I haven't identified a pattern, actually I think\n",
    "it is quite random. This could be due to a variety of reasons, including\n",
    "model parametrization (however, I don't really want to use different \n",
    "parameters than those reported), maybe even unfortunate parameter \n",
    "initialization. Therefore, we will use a technique called \"Early Stopping\".\n",
    "\n",
    "In this case, we will train the model until the number of epochs is reached\n",
    "*or* until the loss is an `NaN`. After that, training is stopped. Then,\n",
    "we will selected and use the model that yielded the smallest lost.\n",
    "\n",
    "We can achieve this very easily using [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.TerminateOnNaN(),\n",
    "             tf.keras.callbacks.ModelCheckpoint(str(PATH_MODELS/f'{example_file}.h5'), monitor='loss', save_best_only=True, mode='min')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "Now we can fit the DeepSurv model. Notice how we use the whole set of \n",
    "patients in a batch. Furthermore, be sure that `shuffle` is set to `False`, \n",
    "since order is important in predicting ranked survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=n_patients_train, \n",
    "                    epochs=epochs, \n",
    "                    callbacks=callbacks,\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the loss changed with the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=[5, 5])\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "ax.set_xlabel(\"No. epochs\")\n",
    "ax.set_ylabel(\"Loss [u.a.]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we saved the model with the lowest loss value (i.e., Early Stop).\n",
    "Now, we need to load it. Since we defined our own custom function,\n",
    "it is important to [use the `compile=False` parameter](https://github.com/keras-team/keras/issues/5916#issuecomment-592269254)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(PATH_MODELS/f'{example_file}.h5', compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions\n",
    "Finally, we can generate predictions using the DeepSurv model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train = np.exp(-model.predict(X_train))\n",
    "c_index_train = utils.concordance_index(Y_train, Y_pred_train, E_train)\n",
    "print(f\"c-index of training dataset = {c_index_train}\")\n",
    "\n",
    "Y_pred_test = np.exp(-model.predict(X_test))\n",
    "c_index_test = utils.concordance_index(Y_test, Y_pred_test, E_test)\n",
    "print(f\"c-index of testing dataset = {c_index_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these numbers are within the ballpark estimate of what is\n",
    "reported in the original paper for this dataset (0.86-0.87, Table 1, p. 6)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
